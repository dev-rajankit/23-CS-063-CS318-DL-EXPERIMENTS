{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "173f1dda",
      "metadata": {
        "id": "173f1dda"
      },
      "source": [
        "# Experiment 4 Text Generation RNN/LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "db48acdb",
      "metadata": {
        "id": "db48acdb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5cbc2345",
      "metadata": {
        "id": "5cbc2345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb8d173-2fa7-4b2d-ce54-2518559ffe56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 24734\n",
            "Vocabulary: 6989\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/poems-100.csv\")\n",
        "\n",
        "text = \" \".join(df.iloc[:,0].astype(str)).lower()\n",
        "words = text.split()\n",
        "\n",
        "vocab = list(set(words))\n",
        "w2i = {w:i for i,w in enumerate(vocab)}\n",
        "i2w = {i:w for w,i in w2i.items()}\n",
        "\n",
        "data = np.array([w2i[w] for w in words])\n",
        "\n",
        "print(\"Total Words:\", len(words))\n",
        "print(\"Vocabulary:\", len(vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9690a3cc",
      "metadata": {
        "id": "9690a3cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cbfda8-ff24-424c-f17f-0b40dd791d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples: 4995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-865247540.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  X = torch.tensor(X)\n"
          ]
        }
      ],
      "source": [
        "MAX_WORDS = 5000\n",
        "data = data[:MAX_WORDS]\n",
        "\n",
        "seq = 5\n",
        "X,Y = [],[]\n",
        "\n",
        "for i in range(len(data)-seq):\n",
        "    X.append(data[i:i+seq])\n",
        "    Y.append(data[i+seq])\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "V = len(vocab)\n",
        "\n",
        "print(\"Samples:\",len(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1321dc39",
      "metadata": {
        "id": "1321dc39"
      },
      "outputs": [],
      "source": [
        "class OneHot(nn.Module):\n",
        "    def __init__(self,cell):\n",
        "        super().__init__()\n",
        "        self.rnn = cell(V,32,batch_first=True)\n",
        "        self.fc = nn.Linear(32,V)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = nn.functional.one_hot(x,V).float()\n",
        "        o,_ = self.rnn(x)\n",
        "        return self.fc(o[:,-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f3f711a8",
      "metadata": {
        "id": "f3f711a8"
      },
      "outputs": [],
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self,cell):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(V,32)\n",
        "        self.rnn = cell(32,32,batch_first=True)\n",
        "        self.fc = nn.Linear(32,V)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.emb(x)\n",
        "        o,_ = self.rnn(x)\n",
        "        return self.fc(o[:,-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eb11048f",
      "metadata": {
        "id": "eb11048f"
      },
      "outputs": [],
      "source": [
        "def train(m):\n",
        "    opt = optim.Adam(m.parameters(),0.01)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for e in range(3):\n",
        "        opt.zero_grad()\n",
        "        out = m(X)\n",
        "        loss = loss_fn(out,Y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        print(\"Epoch\",e+1,\"Loss:\",loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8105a998",
      "metadata": {
        "id": "8105a998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c758b23e-863e-4471-af96-1382c858dd70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RNN OneHot\n",
            "Epoch 1 Loss: 8.861047744750977\n",
            "Epoch 2 Loss: 8.728211402893066\n",
            "Epoch 3 Loss: 8.555278778076172\n",
            "\n",
            " LSTM OneHot\n",
            "Epoch 1 Loss: 8.869963645935059\n",
            "Epoch 2 Loss: 8.815950393676758\n",
            "Epoch 3 Loss: 8.751444816589355\n",
            "\n",
            " RNN Embed\n",
            "Epoch 1 Loss: 8.892857551574707\n",
            "Epoch 2 Loss: 8.743245124816895\n",
            "Epoch 3 Loss: 8.586085319519043\n",
            "\n",
            " LSTM Embed\n",
            "Epoch 1 Loss: 8.862014770507812\n",
            "Epoch 2 Loss: 8.798221588134766\n",
            "Epoch 3 Loss: 8.72358226776123\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "    \"RNN OneHot\": OneHot(nn.RNN),\n",
        "    \"LSTM OneHot\": OneHot(nn.LSTM),\n",
        "    \"RNN Embed\": Embed(nn.RNN),\n",
        "    \"LSTM Embed\": Embed(nn.LSTM)\n",
        "}\n",
        "\n",
        "trained = {}\n",
        "\n",
        "for k,m in models.items():\n",
        "    print(\"\\n\",k)\n",
        "    train(m)\n",
        "    trained[k] = m\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(m,start):\n",
        "    m.eval()\n",
        "    w = start.split()\n",
        "\n",
        "    for _ in range(20):\n",
        "        t = torch.tensor([[w2i[i] for i in w[-seq:]]])\n",
        "        nxt = m(t).argmax().item()\n",
        "        w.append(i2w[nxt])\n",
        "\n",
        "    return \" \".join(w)\n",
        "\n",
        "for k,m in trained.items():\n",
        "    print(\"\\n\",k)\n",
        "    print(generate(m,\"the sun is\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD5ph0EIl0-S",
        "outputId": "c366f907-ca3e-4d33-a6c3-020e8c17b957"
      },
      "id": "BD5ph0EIl0-S",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " RNN OneHot\n",
            "the sun is crowded time time time and and and and and and and and and and and and and and and and\n",
            "\n",
            " LSTM OneHot\n",
            "the sun is spread spread spread spread spread spread spread spread spread spread spread spread spread spread spread spread spread spread spread spread\n",
            "\n",
            " RNN Embed\n",
            "the sun is fry. youth lived began life burn! self or tired, dreams death. marked old associates, older meat can follow glory-garland atom\n",
            "\n",
            " LSTM Embed\n",
            "the sun is jesus’ jesus’ mean; supple use dear) pass, saved gang dinner, mean; use dear) but but melt none heaven smoke i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "1. One-Hot models consume more memory and train slower.\n",
        "\n",
        "2. Embedding models converge faster and produce smoother sentences.\n",
        "\n",
        "3. RNN struggles with long context.\n",
        "\n",
        "4. LSTM captures better sequence dependencies.\n",
        "\n",
        "Conclusion:\n",
        "LSTM with Trainable Embeddings gives best performance and text quality.\n"
      ],
      "metadata": {
        "id": "4RD9DKp5mDbd"
      },
      "id": "4RD9DKp5mDbd"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}