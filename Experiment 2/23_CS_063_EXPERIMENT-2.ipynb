{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b7fbfb",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network from Scratch (NumPy Only) â€” Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e7411",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cbeb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b12bb4",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1e7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 60000 Validation: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_dataset), \"Validation:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f800b",
   "metadata": {},
   "source": [
    "## Convert Torch to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd7076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def torch_to_numpy(loader):\n",
    "    X_list, y_list = [], []\n",
    "    for images, labels in loader:\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        X_list.append(images.numpy().reshape(images.shape[0], -1))\n",
    "        y_list.append(labels.numpy())\n",
    "    return np.vstack(X_list), np.hstack(y_list)\n",
    "\n",
    "X_train, y_train = torch_to_numpy(train_loader)\n",
    "X_val, y_val = torch_to_numpy(val_loader)\n",
    "\n",
    "def one_hot(y, c=10):\n",
    "    out = np.zeros((y.size, c))\n",
    "    out[np.arange(y.size), y] = 1\n",
    "    return out\n",
    "\n",
    "y_train_oh = one_hot(y_train)\n",
    "y_val_oh = one_hot(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e010d1",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fa91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(z): return np.maximum(0, z)\n",
    "def relu_derivative(z): return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)\n",
    "\n",
    "def tanh(z): return np.tanh(z)\n",
    "def tanh_derivative(z): return 1 - np.tanh(z)**2\n",
    "\n",
    "def softmax(z):\n",
    "    e = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == \"relu\": return relu, relu_derivative\n",
    "    if name == \"sigmoid\": return sigmoid, sigmoid_derivative\n",
    "    if name == \"tanh\": return tanh, tanh_derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322a1ba",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350ed457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation='relu', output_activation='softmax', lr=0.05):\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "        self.params = {}\n",
    "        self.cache = {}\n",
    "        self.act, self.act_der = get_activation(activation)\n",
    "        self.output_activation = output_activation\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            self.params['W'+str(i)] = np.random.randn(self.layers[i], self.layers[i+1]) * 0.01\n",
    "            self.params['b'+str(i)] = np.zeros((1, self.layers[i+1]))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache['A0'] = X\n",
    "        L = len(self.layers)-1\n",
    "        \n",
    "        for i in range(L-1):\n",
    "            Z = self.cache['A'+str(i)] @ self.params['W'+str(i)] + self.params['b'+str(i)]\n",
    "            A = self.act(Z)\n",
    "            self.cache['Z'+str(i+1)] = Z\n",
    "            self.cache['A'+str(i+1)] = A\n",
    "        \n",
    "        ZL = self.cache['A'+str(L-1)] @ self.params['W'+str(L-1)] + self.params['b'+str(L-1)]\n",
    "        AL = softmax(ZL) if self.output_activation==\"softmax\" else self.act(ZL)\n",
    "        self.cache['A'+str(L)] = AL\n",
    "        return AL\n",
    "    \n",
    "    def compute_loss(self, Y_pred, Y_true):\n",
    "        eps = 1e-9\n",
    "        return -np.mean(np.sum(Y_true*np.log(Y_pred+eps), axis=1))\n",
    "    \n",
    "    def backward(self, Y_true):\n",
    "        grads = {}\n",
    "        L = len(self.layers)-1\n",
    "        m = Y_true.shape[0]\n",
    "        \n",
    "        dZ = self.cache['A'+str(L)] - Y_true\n",
    "        \n",
    "        for i in reversed(range(L)):\n",
    "            A_prev = self.cache['A'+str(i)]\n",
    "            grads['dW'+str(i)] = A_prev.T @ dZ / m\n",
    "            grads['db'+str(i)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            if i > 0:\n",
    "                dA = dZ @ self.params['W'+str(i)].T\n",
    "                dZ = dA * self.act_der(self.cache['Z'+str(i)])\n",
    "        \n",
    "        self.grads = grads\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        for k in self.params:\n",
    "            self.params[k] -= self.lr * self.grads['d'+k]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        return np.mean(self.predict(X) == np.argmax(Y, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160a1fd",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4ab632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, Xtr, Ytr, Xv, Yv, epochs=5, batch_size=256):\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
    "    n = Xtr.shape[0]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        perm = np.random.permutation(n)\n",
    "        Xtr, Ytr = Xtr[perm], Ytr[perm]\n",
    "        \n",
    "        for i in range(0, n, batch_size):\n",
    "            xb = Xtr[i:i+batch_size]\n",
    "            yb = Ytr[i:i+batch_size]\n",
    "            model.forward(xb)\n",
    "            model.backward(yb)\n",
    "            model.update_parameters()\n",
    "        \n",
    "        train_pred = model.forward(Xtr)\n",
    "        val_pred = model.forward(Xv)\n",
    "        \n",
    "        history['train_loss'].append(model.compute_loss(train_pred, Ytr))\n",
    "        history['val_loss'].append(model.compute_loss(val_pred, Yv))\n",
    "        history['train_acc'].append(model.evaluate(Xtr, Ytr))\n",
    "        history['val_acc'].append(model.evaluate(Xv, Yv))\n",
    "        \n",
    "        print(f\"Epoch {e+1} | TrainAcc={history['train_acc'][-1]:.3f} | ValAcc={history['val_acc'][-1]:.3f}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03fb22",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b2d1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: {'layers': [784, 128, 10], 'activation': 'relu'}\n",
      "Epoch 1 | TrainAcc=0.815 | ValAcc=0.822\n",
      "Epoch 2 | TrainAcc=0.875 | ValAcc=0.881\n",
      "Epoch 3 | TrainAcc=0.894 | ValAcc=0.897\n",
      "Epoch 4 | TrainAcc=0.902 | ValAcc=0.905\n",
      "Epoch 5 | TrainAcc=0.909 | ValAcc=0.913\n",
      "\n",
      "Running: {'layers': [784, 128, 64, 10], 'activation': 'relu'}\n",
      "Epoch 1 | TrainAcc=0.112 | ValAcc=0.114\n",
      "Epoch 2 | TrainAcc=0.112 | ValAcc=0.114\n",
      "Epoch 3 | TrainAcc=0.374 | ValAcc=0.378\n",
      "Epoch 4 | TrainAcc=0.742 | ValAcc=0.749\n",
      "Epoch 5 | TrainAcc=0.812 | ValAcc=0.818\n",
      "\n",
      "Running: {'layers': [784, 256, 128, 10], 'activation': 'relu'}\n",
      "Epoch 1 | TrainAcc=0.112 | ValAcc=0.114\n",
      "Epoch 2 | TrainAcc=0.370 | ValAcc=0.377\n",
      "Epoch 3 | TrainAcc=0.754 | ValAcc=0.765\n",
      "Epoch 4 | TrainAcc=0.813 | ValAcc=0.822\n",
      "Epoch 5 | TrainAcc=0.861 | ValAcc=0.861\n",
      "\n",
      "Running: {'layers': [784, 128, 10], 'activation': 'sigmoid'}\n",
      "Epoch 1 | TrainAcc=0.158 | ValAcc=0.159\n",
      "Epoch 2 | TrainAcc=0.551 | ValAcc=0.562\n",
      "Epoch 3 | TrainAcc=0.715 | ValAcc=0.722\n",
      "Epoch 4 | TrainAcc=0.786 | ValAcc=0.788\n",
      "Epoch 5 | TrainAcc=0.821 | ValAcc=0.825\n",
      "\n",
      "Running: {'layers': [784, 128, 10], 'activation': 'tanh'}\n",
      "Epoch 1 | TrainAcc=0.820 | ValAcc=0.826\n",
      "Epoch 2 | TrainAcc=0.879 | ValAcc=0.884\n",
      "Epoch 3 | TrainAcc=0.894 | ValAcc=0.898\n",
      "Epoch 4 | TrainAcc=0.902 | ValAcc=0.906\n",
      "Epoch 5 | TrainAcc=0.909 | ValAcc=0.913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Final Train Acc</th>\n",
       "      <th>Final Val Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_0_relu_1hidden</td>\n",
       "      <td>[784, 128, 10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.908983</td>\n",
       "      <td>0.9126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_1_relu_2hidden</td>\n",
       "      <td>[784, 128, 64, 10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.811633</td>\n",
       "      <td>0.8175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_2_relu_2hidden</td>\n",
       "      <td>[784, 256, 128, 10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.861067</td>\n",
       "      <td>0.8612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_3_sigmoid_1hidden</td>\n",
       "      <td>[784, 128, 10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.820517</td>\n",
       "      <td>0.8247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exp_4_tanh_1hidden</td>\n",
       "      <td>[784, 128, 10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.909283</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Experiment               Layers Activation  Final Train Acc  \\\n",
       "0     exp_0_relu_1hidden       [784, 128, 10]       relu         0.908983   \n",
       "1     exp_1_relu_2hidden   [784, 128, 64, 10]       relu         0.811633   \n",
       "2     exp_2_relu_2hidden  [784, 256, 128, 10]       relu         0.861067   \n",
       "3  exp_3_sigmoid_1hidden       [784, 128, 10]    sigmoid         0.820517   \n",
       "4     exp_4_tanh_1hidden       [784, 128, 10]       tanh         0.909283   \n",
       "\n",
       "   Final Val Acc  \n",
       "0         0.9126  \n",
       "1         0.8175  \n",
       "2         0.8612  \n",
       "3         0.8247  \n",
       "4         0.9127  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "experiments = [\n",
    "    {\"layers\":[784,128,10], \"activation\":\"relu\"},\n",
    "    {\"layers\":[784,128,64,10], \"activation\":\"relu\"},\n",
    "    {\"layers\":[784,256,128,10], \"activation\":\"relu\"},\n",
    "    {\"layers\":[784,128,10], \"activation\":\"sigmoid\"},\n",
    "    {\"layers\":[784,128,10], \"activation\":\"tanh\"},\n",
    "]\n",
    "\n",
    "EPOCHS = 5\n",
    "results = []\n",
    "\n",
    "for idx, exp in enumerate(experiments):\n",
    "    print(\"\\nRunning:\", exp)\n",
    "    \n",
    "    model = NeuralNetwork(exp[\"layers\"], activation=exp[\"activation\"], output_activation=\"softmax\", lr=0.05)\n",
    "    history = train_model(model, X_train, y_train_oh, X_val, y_val_oh, epochs=EPOCHS)\n",
    "    \n",
    "    exp_name = f\"exp_{idx}_{exp['activation']}_{len(exp['layers'])-2}hidden\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['train_loss'], label=\"Train\")\n",
    "    plt.plot(history['val_loss'], label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.title(exp_name+\" Loss\")\n",
    "    plt.savefig(f\"results/{exp_name}_loss.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['train_acc'], label=\"Train\")\n",
    "    plt.plot(history['val_acc'], label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.title(exp_name+\" Accuracy\")\n",
    "    plt.savefig(f\"results/{exp_name}_accuracy.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    results.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Layers\": exp[\"layers\"],\n",
    "        \"Activation\": exp[\"activation\"],\n",
    "        \"Final Train Acc\": history[\"train_acc\"][-1],\n",
    "        \"Final Val Acc\": history[\"val_acc\"][-1]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"results/experiment_results.csv\", index=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515053cf",
   "metadata": {},
   "source": [
    "## View Saved Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb51ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experiment_results.csv',\n",
       " 'exp_0_relu_1hidden_accuracy.png',\n",
       " 'exp_0_relu_1hidden_loss.png',\n",
       " 'exp_1_relu_2hidden_accuracy.png',\n",
       " 'exp_1_relu_2hidden_loss.png',\n",
       " 'exp_2_relu_2hidden_accuracy.png',\n",
       " 'exp_2_relu_2hidden_loss.png',\n",
       " 'exp_3_sigmoid_1hidden_accuracy.png',\n",
       " 'exp_3_sigmoid_1hidden_loss.png',\n",
       " 'exp_4_tanh_1hidden_accuracy.png',\n",
       " 'exp_4_tanh_1hidden_loss.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.listdir(\"results\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
