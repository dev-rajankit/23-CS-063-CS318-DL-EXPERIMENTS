{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e91045c",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network from Scratch (NumPy Only)\n",
    "\n",
    "PyTorch is used only to load MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e404e7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1416c64",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813d094",
   "metadata": {},
   "source": [
    "## Convert Torch Tensor to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def torch_to_numpy(loader):\n",
    "    X_list, y_list = [], []\n",
    "    for images, labels in loader:\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        X_list.append(images.numpy().reshape(images.shape[0], -1))\n",
    "        y_list.append(labels.numpy())\n",
    "    return np.vstack(X_list), np.hstack(y_list)\n",
    "\n",
    "X_train, y_train = torch_to_numpy(train_loader)\n",
    "X_val, y_val = torch_to_numpy(val_loader)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "def one_hot(y, c=10):\n",
    "    out = np.zeros((y.size, c))\n",
    "    out[np.arange(y.size), y] = 1\n",
    "    return out\n",
    "\n",
    "y_train_oh = one_hot(y_train)\n",
    "y_val_oh = one_hot(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4627c",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(z): return np.maximum(0, z)\n",
    "def relu_derivative(z): return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "def sigmoid_derivative(z): \n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)\n",
    "\n",
    "def tanh(z): return np.tanh(z)\n",
    "def tanh_derivative(z): return 1-np.tanh(z)**2\n",
    "\n",
    "def softmax(z):\n",
    "    e = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496b8e9",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, activation='relu', lr=0.01):\n",
    "        self.layers = layers\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.params = {}\n",
    "        self.cache = {}\n",
    "        self.init_params()\n",
    "        \n",
    "    def init_params(self):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            self.params['W'+str(i)] = np.random.randn(self.layers[i], self.layers[i+1])*0.01\n",
    "            self.params['b'+str(i)] = np.zeros((1, self.layers[i+1]))\n",
    "    \n",
    "    def activate(self, z):\n",
    "        return relu(z) if self.activation=='relu' else sigmoid(z) if self.activation=='sigmoid' else tanh(z)\n",
    "    \n",
    "    def activate_derivative(self, z):\n",
    "        return relu_derivative(z) if self.activation=='relu' else sigmoid_derivative(z) if self.activation=='sigmoid' else tanh_derivative(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache['A0'] = X\n",
    "        L = len(self.layers)-1\n",
    "        \n",
    "        for i in range(L-1):\n",
    "            Z = self.cache['A'+str(i)] @ self.params['W'+str(i)] + self.params['b'+str(i)]\n",
    "            A = self.activate(Z)\n",
    "            self.cache['Z'+str(i+1)] = Z\n",
    "            self.cache['A'+str(i+1)] = A\n",
    "        \n",
    "        ZL = self.cache['A'+str(L-1)] @ self.params['W'+str(L-1)] + self.params['b'+str(L-1)]\n",
    "        AL = softmax(ZL)\n",
    "        self.cache['A'+str(L)] = AL\n",
    "        return AL\n",
    "    \n",
    "    def compute_loss(self, Y_pred, Y_true):\n",
    "        eps = 1e-9\n",
    "        return -np.mean(np.sum(Y_true*np.log(Y_pred+eps), axis=1))\n",
    "    \n",
    "    def backward(self, Y_true):\n",
    "        grads = {}\n",
    "        L = len(self.layers)-1\n",
    "        m = Y_true.shape[0]\n",
    "        \n",
    "        dZ = self.cache['A'+str(L)] - Y_true\n",
    "        \n",
    "        for i in reversed(range(L)):\n",
    "            A_prev = self.cache['A'+str(i)]\n",
    "            grads['dW'+str(i)] = A_prev.T @ dZ / m\n",
    "            grads['db'+str(i)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            \n",
    "            if i > 0:\n",
    "                dA = dZ @ self.params['W'+str(i)].T\n",
    "                dZ = dA * self.activate_derivative(self.cache['Z'+str(i)])\n",
    "        \n",
    "        self.grads = grads\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        for k in self.params:\n",
    "            self.params[k] -= self.lr * self.grads['d'+k]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        return np.mean(self.predict(X) == np.argmax(Y, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74353769",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, Xtr, Ytr, Xv, Yv, epochs=5, batch_size=128):\n",
    "    hist = {'loss':[], 'val_acc':[]}\n",
    "    n = Xtr.shape[0]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        perm = np.random.permutation(n)\n",
    "        Xtr, Ytr = Xtr[perm], Ytr[perm]\n",
    "        \n",
    "        for i in range(0,n,batch_size):\n",
    "            xb = Xtr[i:i+batch_size]\n",
    "            yb = Ytr[i:i+batch_size]\n",
    "            pred = model.forward(xb)\n",
    "            model.backward(yb)\n",
    "            model.update_parameters()\n",
    "        \n",
    "        loss = model.compute_loss(model.forward(Xtr), Ytr)\n",
    "        acc = model.evaluate(Xv, Yv)\n",
    "        hist['loss'].append(loss)\n",
    "        hist['val_acc'].append(acc)\n",
    "        print(f\"Epoch {e+1} Loss={loss:.4f} ValAcc={acc:.4f}\")\n",
    "    return hist\n",
    "\n",
    "model = NeuralNetwork([784,128,64,10], activation='relu', lr=0.1)\n",
    "history = train(model, X_train, y_train_oh, X_val, y_val_oh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745e261",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(history['loss'])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['val_acc'])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
